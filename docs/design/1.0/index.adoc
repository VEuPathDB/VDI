= VDI Design
:icons: font
:toc: left

== Components

image::assets/vdi-components.svg[]

== Workflow

image::assets/vdi-overall-flow.svg[]

=== REST API Actions

==== List Datasets for User

==== Lookup Dataset

==== Create Dataset

image::assets/vdi-user-action-create.svg[]

. Client ``POST``s a dataset and accompanying metadata to the VDI REST API.
. REST Service sanity-checks the dataset metadata.
. REST Service generates a VDI ID for the dataset.
. REST Service writes records into the cache DB for the dataset.
** dataset details
** dataset metadata
** dataset import status
** dataset project links
. REST service ``PUT``s the dataset data into the VDI MinIO bucket.
. MinIO emits an event for the ``PUT`` to the appropriate Kafka topic.

[IMPORTANT]
--
Seems like the REST service is doing too much in this case.  Perhaps the REST
service should simply transfer data to the MinIO bucket and the writing of
metadata to the database should be handled on the other side of the Kafka topic?
--

==== Update Dataset Metadata

image::assets/vdi-user-action-update.svg[]

. Client submits ``PATCH`` request with updates to the dataset's metadata.
. REST Service sanity checks the dataset metadata.
. REST Service updates the metadata in the Cache DB
. REST Service ``PUT``s the updated metadata into the VDI Bucket (MinIO)
. MinIO emits an event for the ``PUT`` to the appropriate Kafka topic

==== Delete Dataset

image::assets/vdi-user-action-delete.svg[]

. Client submits ``DELETE`` request to the target dataset.
. REST service validates that the requesting user owns the target dataset.
. REST service updates the dataset record in the Cache DB, marking it as
  "deleted"
. REST service ``PUT``s a delete marker flag in the dataset "directory" in the
  VDI Bucket (MinIO).
. MinIO emits an event for the ``PUT`` to the appropriate Kafka topic.

==== Offer Dataset Share

image::assets/vdi-user-action-share-offer.svg[]

. REST Service validates target user exists.
. REST service creates share record in Cache DB
. REST service puts share objects into MinIO
.. Share Offer File
.. Share Receipt File
. MinIO emits events for the ``PUT`` to the appropriate Kafka topic

==== Revoke Dataset Share Offer

image::assets/vdi-user-action-share-revoke.svg[]

==== Accept Dataset Share Offer

image::assets/vdi-user-action-share-accept.svg[]

==== Reject Dataset Share Offer

image::assets/vdi-user-action-share-reject.svg[]

==== Reconciliation

==== Failed Dataset Install Cleanup

==== Deleted Dataset Cleanup

=== Internal Actions

==== Import Dataset

image::assets/vdi-internal-action-import.svg[]

==== Install Dataset

==== Update Dataset Metadata

==== Soft Delete Dataset

==== Hard Delete Dataset

== Resources

=== Global Resources

==== MinIO

link:https://min.io/[MinIO] is an link:https://aws.amazon.com/s3/[Amazon S3]
compatible object store that can be hosted and managed on-site.

VDI uses a single bucket (object container) with object keys structured as if
they were filesystem paths to the target objects.  With these key/paths we
create conceptual "directories" to "contain" the dataset files.

The structure of the paths is as follows:

[source, directory-tree]
----
bucket/
  |- {owner-user-id}/
  |    |- {dataset-id}/
  |    |    |- data/
  |    |    |    |- some-data-file-1.tsv
  |    |    |    |- some-data-file-2.tsv
  |    |    |- shares/
  |    |    |    |- {recipient-user-id}/
  |    |    |    |    |- offer.json
  |    |    |    |    |- receipt.json
  |    |    |- upload/
  |    |    |    |- uploaded-dataset-file-1.png
  |    |    |    |- uploaded-dataset-file-2.xml
  |    |    |- delete-flag
  |    |    |- manifest.json
  |    |    |- meta.json
----

With this structure it is easy to reason about and crawl the contents of the
VDI bucket

==== Apache Kafka

==== Oracle Account DB

==== Oracle User DB(s)

==== Oracle Application DBs

=== Project Resources

==== Cache DB

===== Schema