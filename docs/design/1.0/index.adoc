= VDI Design
:icons: font
:toc: left

== Components

image::assets/vdi-components.svg[]

== Workflow

image::assets/vdi-overall-flow.svg[]

=== REST API Actions

==== List Datasets for User

==== Lookup Dataset

==== Create Dataset

image::assets/vdi-user-action-create.svg[]

. Client ``POST``s a dataset and accompanying metadata to the VDI REST API.
. REST Service sanity-checks the dataset metadata.
. REST Service generates a VDI ID for the dataset.
. REST Service writes records into the cache DB for the dataset.
** dataset details
** dataset metadata
** dataset import status
** dataset project links
. REST service ``PUT``s the dataset data into the VDI MinIO bucket.
. MinIO emits an event for the ``PUT`` to the appropriate Kafka topic.

[IMPORTANT]
--
Seems like the REST service is doing too much in this case.  Perhaps the REST
service should simply transfer data to the MinIO bucket and the writing of
metadata to the database should be handled on the other side of the Kafka topic?
--

==== Update Dataset Metadata

image::assets/vdi-user-action-update.svg[]

. Client submits ``PATCH`` request with updates to the dataset's metadata.
. REST Service sanity checks the dataset metadata.
. REST Service updates the metadata in the Cache DB
. REST Service ``PUT``s the updated metadata into the VDI Bucket (MinIO)
. MinIO emits an event for the ``PUT`` to the appropriate Kafka topic

==== Delete Dataset

image::assets/vdi-user-action-delete.svg[]

. Client submits ``DELETE`` request to the target dataset.
. REST service validates that the requesting user owns the target dataset.
. REST service updates the dataset record in the Cache DB, marking it as
  "deleted"
. REST service ``PUT``s a delete marker flag in the dataset "directory" in the
  VDI Bucket (MinIO).
. MinIO emits an event for the ``PUT`` to the appropriate Kafka topic.

==== Offer Dataset Share

image::assets/vdi-user-action-share-offer.svg[]

. REST Service validates target user exists.
. REST service creates share record in Cache DB
. REST service puts share objects into MinIO
.. Share Offer File
.. Share Receipt File
. MinIO emits events for the ``PUT`` to the appropriate Kafka topic

==== Revoke Dataset Share Offer

image::assets/vdi-user-action-share-revoke.svg[]

==== Accept Dataset Share Offer

image::assets/vdi-user-action-share-accept.svg[]

==== Reject Dataset Share Offer

image::assets/vdi-user-action-share-reject.svg[]

==== Reconciliation

==== Failed Dataset Install Cleanup

==== Deleted Dataset Cleanup

=== Internal Actions

==== Import Dataset

image::assets/vdi-internal-action-import.svg[]

==== Install Dataset

==== Update Dataset Metadata

==== Soft Delete Dataset

==== Hard Delete Dataset

== Resources

=== Global Resources

==== MinIO

VDI uses a single bucket (object container) with object keys structured as if
they were filesystem paths to the target objects.  With these key/paths we
create conceptual "directories" to "contain" the dataset files.

The structure of the paths is as follows:

[source, directory-tree]
----
bucket/
  |- {owner-user-id}/
  |    |- {dataset-id}/
  |    |    |- data/
  |    |    |    |- some-data-file-1.tsv
  |    |    |    |- some-data-file-2.tsv
  |    |    |- shares/
  |    |    |    |- {recipient-user-id}/
  |    |    |    |    |- offer.json
  |    |    |    |    |- receipt.json
  |    |    |- upload/
  |    |    |    |- uploaded-dataset-file-1.png
  |    |    |    |- uploaded-dataset-file-2.xml
  |    |    |- delete-flag
  |    |    |- manifest.json
  |    |    |- meta.json
----

With this structure it is easy to reason about and crawl the contents of the
VDI bucket

==== Rabbit MQ

VDI uses RabbitMQ to listen for event messages coming from <<MinIO>> that
represent object creations and deletions within the VDI MinIO bucket.  Every
time an object is put into the bucket or deleted from the bucket an event
message is sent through RabbitMQ.

==== Oracle Account DB

==== Oracle User DB(s)

==== Oracle Application DBs

=== Project Resources

==== Apache Kafka

==== Cache DB

== Database Schemata

=== Internal Cache DB

==== `vdi.datasets`

[%header, cols="3m,1m,6"]
|===
| Column       | Type      | Comment
| dataset_id   | CHAR(32)  |
| type_name    | VARCHAR   | Name of the dataset type.
| type_version | VARCHAR   | Version for the dataset type.
| owner_id     | VARCHAR   | User ID of the owner of the dataset.  WDK user IDs will be `long` values.
| is_deleted   | BOOLEAN   | Soft delete marker.
| created      | TIMESTAMP |
|===


==== `vdi.dataset_files`

[%header, cols="3m,1m,6"]
|===
| Column     | Type     | Comment
| dataset_id | CHAR(32) |
| file_name  | VARCHAR  |
|===


==== `vdi.dataset_projects`

[%header, cols="3m,1m,6"]
|===
| Column     | Type     | Comment
| dataset_id | CHAR(32) |
| project_id | VARCHAR  |
|===


==== `vdi.dataset_metadata`

[%header, cols="3m,1m,6"]
|===
| Column      | Type     | Comment
| dataset_id  | CHAR(32) |
| name        | VARCHAR  | Name of the dataset.
| summary     | VARCHAR  | Optional summary for the dataset.
| description | VARCHAR  | Optional description of the dataset.
|===


==== `vdi.sync_control`

[%header, cols="3m,1m,6"]
|===
| Column             | Type      | Comment
| dataset_id         | CHAR(32)  |
| shares_update_time | TIMESTAMP | Timestamp of the most recent share file across all shares.
| data_update_time   | TIMESTAMP | Timestamp of the most recent data file
| meta_update_time   | TIMESTAMP | Timestamp of the meta file
|===


==== `vdi.dataset_share_offers`

[%header, cols="3m,1m,6"]
|===
| Column             | Type      | Comment
| dataset_id         | CHAR(32)  |
| recipient_id       | VARCHAR   | ID of the recipient of the share offer.
| status             | VARCHAR   | `"grant" \| "revoke"`
|===


==== `vdi.dataset_share_receipts`

[%header, cols="3m,1m,6"]
|===
| Column             | Type      | Comment
| dataset_id         | CHAR(32)  |
| recipient_id       | VARCHAR   | ID of the recipient of the share offer.
| status             | VARCHAR   | `"accept" \| "reject"`
|===


==== `vdi.import_control`

[%header, cols="3m,1m,6"]
|===
| Column             | Type      | Comment
| dataset_id         | CHAR(32)  |
| status             | VARCHAR   | `"awaiting_import" \| "importing" \| "imported" \| "failed"`
|===


=== Application DBs